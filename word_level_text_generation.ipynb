{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level text generation\n",
    "### Imports, utils, and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from textblob import TextBlob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_embeddings(token_lists, embedding_file_path,\n",
    "                   min_vocab_size=0, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Build emeddding mappings based on `texts`.\n",
    "\n",
    "    Args:\n",
    "        token_lists: A sequence of tokens to fit the embeddings\n",
    "            on.\n",
    "        embedding_file_path: The full path to a pretrained embeddings\n",
    "            file.\n",
    "        min_vocab_size: Minimum number of common words to include,\n",
    "            even if they don't appear in `token_lists`. Defaults to\n",
    "            `0`.\n",
    "        max_vocab_size: Maximum number of words from the data set to\n",
    "            include. Defaults to `None`.\n",
    "\n",
    "    Returns:\n",
    "        token_to_vec: A `dict` mapping between word tokens and numpy vectors.\n",
    "        token_to_id: A `dict` mapping between word tokens and embedding ids.\n",
    "        embedding_matrix: A numpy `ndarray` that maps between embedding ids\n",
    "            and embedding vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    vocab_counter = Counter(chain(*token_lists))\n",
    "    most_common_counts = vocab_counter.most_common(max_vocab_size)\n",
    "    data_vocab = set(token for token, _ in most_common_counts)\n",
    "\n",
    "    token_to_vec = {}\n",
    "    with open(embedding_file_path) as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            values = line.split()  # Splits on spaces.\n",
    "            word = values[0]\n",
    "            if min_vocab_size < line_num + 1 and word not in data_vocab:\n",
    "                continue\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            token_to_vec[word] = vector\n",
    "\n",
    "    vocab = data_vocab | set(token_to_vec.keys())\n",
    "    rand_state = np.random.RandomState(42)\n",
    "    token_to_id = {'<PAD>': 0, '<EOS>': 1, '<START>': 2, '<UNK>': 3}\n",
    "    num_meta_tokens = len(token_to_id)\n",
    "    embedding_matrix = rand_state.rand(len(vocab) + num_meta_tokens, 200)\n",
    "    for i in range(num_meta_tokens):\n",
    "        embedding_matrix[i] = i\n",
    "    vocab = sorted(vocab)  # Sort for consistent ids.\n",
    "    for i, token in enumerate(vocab):\n",
    "        word_id = i + num_meta_tokens\n",
    "        if token in token_to_vec:\n",
    "            embedding_matrix[word_id] = token_to_vec[token]\n",
    "        token_to_id[token] = word_id\n",
    "\n",
    "    return token_to_vec, token_to_id, embedding_matrix\n",
    "\n",
    "\n",
    "def tokens_to_ids(token_lists, token_to_id, max_sequence_length=None,\n",
    "                  add_eos=False, skip_unknown=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        texts: A sequence of texts to fit the embeddings\n",
    "            on.\n",
    "        token_to_id: A `dict` map between word tokens and embedding ids.\n",
    "        max_sequence_length: The maximum number of words to include in\n",
    "            each line of dialogue. Shorter sequences will be padded with\n",
    "            the <PAD> vector. Defaults to `None`.\n",
    "        add_eos: Whether or not to add an <EOS> token.\n",
    "        skip_unknown: Whether or not to skip unknown tokens. If `False`,\n",
    "            unknown tokens are assigned the `<UNK>` meta-token. Defaults\n",
    "            to `False`.\n",
    "\n",
    "    Returns:\n",
    "        id_matrix: An array with shape `(len(texts), max_sequence_length)`\n",
    "            containing the correct embedding ids.\n",
    "\n",
    "    \"\"\"\n",
    "    id_lists = []\n",
    "    for token_list in token_lists:\n",
    "        token_ids = []\n",
    "        for token in token_list:\n",
    "            if token in token_to_id:\n",
    "                token_ids.append(token_to_id[token])\n",
    "            elif skip_unknown:\n",
    "                token_ids.append(token_to_id['<UNK>'])\n",
    "        if add_eos:\n",
    "            token_ids.append(token_to_id['<EOS>'])\n",
    "        id_lists.append(token_ids)\n",
    "\n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(len(ids) for ids in id_lists)\n",
    "\n",
    "    id_matrix = np.zeros((len(token_lists), max_sequence_length), dtype=int)\n",
    "    for i, id_list in enumerate(id_lists):\n",
    "        id_list = id_list[:max_sequence_length]\n",
    "        id_matrix[i, :len(id_list)] = id_list\n",
    "\n",
    "    return id_matrix\n",
    "\n",
    "\n",
    "def get_train_val_test_indices(num_rows, val_ratio=0.25, test_ratio=0.25, seed=42):\n",
    "    \"\"\"Return indices of the train, test, and validation sets.\"\"\"\n",
    "    # Calculate the number of items in test and val sets.\n",
    "    num_val = int(num_rows * val_ratio)\n",
    "    num_test = int(num_rows * test_ratio)\n",
    "\n",
    "    # Slice an array of permuted indices to those sizes.\n",
    "    rand = np.random.RandomState(seed)\n",
    "    indices = rand.permutation(range(num_rows))\n",
    "    val_indices = indices[:num_val]\n",
    "    test_indices = indices[num_val:num_val + num_test]\n",
    "    train_indices = indices[num_val + num_test:]\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "\n",
    "def text_to_tokens(text, seq_length=100):\n",
    "    \"\"\"\n",
    "    Split a string into non-overlapping sequences of tokens.\n",
    "\n",
    "    Args:\n",
    "        text: A `string` to chop up into sequences.\n",
    "        seq_length: The length of each sequence.\n",
    "\n",
    "    Returns:\n",
    "        X_tokens: A `list` of `list`s where each inner list contains\n",
    "            `seq_length` tokens.\n",
    "        y_tokens: A `list` with the same shape as `X_tokens` but where\n",
    "            each word is the word after the word at the same index in\n",
    "            `X_tokens`.\n",
    "\n",
    "    \"\"\"\n",
    "    # Ensure text is lower case.\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Put NEWLINE between the lines. I want them to be\n",
    "    # to be a token the model can predict but the simple\n",
    "    # tokenizer I'm using will strip them out.\n",
    "    text = text.replace('\\n', ' NEWLINE ')\n",
    "    \n",
    "    # Glove has an embedding for the dash, but again the\n",
    "    # simple tokenizer I'm using won't break up words \n",
    "    # connected with dashes.\n",
    "    text = text.replace('—', ' — ')\n",
    "    text = text.replace('-', ' - ')\n",
    "    \n",
    "    tokens = tuple(TextBlob(text).tokens)\n",
    "    num_sequences = (len(tokens) - 1) // seq_length\n",
    "\n",
    "    X_tokens = []\n",
    "    y_tokens = []\n",
    "    for seq_index in range(num_sequences):\n",
    "        sequence_start = seq_index * seq_length\n",
    "        x = tokens[sequence_start:sequence_start + seq_length]\n",
    "        y = tokens[sequence_start + 1: sequence_start + seq_length + 1]\n",
    "        X_tokens.append(x)\n",
    "        y_tokens.append(y)\n",
    "\n",
    "    return X_tokens, y_tokens\n",
    "\n",
    "\n",
    "def ids_to_tokens(ids, id_to_word):\n",
    "    \"\"\"Turn a list of token ids into a list of tokens.\"\"\"\n",
    "    return [id_to_word[id] for id in ids]\n",
    "\n",
    "\n",
    "def id_lists_to_token_lists(id_lists, id_to_word):\n",
    "    \"\"\"Turn a matrix of token ids into a matrix of tokens.\"\"\"\n",
    "    return [ids_to_tokens(ids, id_to_word) for ids in id_lists]\n",
    "\n",
    "\n",
    "def save_checkpoint(model, metric_value, file_path, higher_is_better=True):\n",
    "    \"\"\"Save a checkpoint if performance improved.\"\"\"\n",
    "    try:\n",
    "        old_state_dict = torch.load(file_path)\n",
    "        prev_best = old_state_dict['checkpoint_metric']\n",
    "    except (FileNotFoundError, KeyError):\n",
    "        prev_best = None\n",
    "    if (prev_best is None\n",
    "            or (higher_is_better and prev_best < metric_value)\n",
    "            or (not higher_is_better and prev_best > metric_value)):\n",
    "        state = model.state_dict()\n",
    "        state['checkpoint_metric'] = metric_value\n",
    "        torch.save(state, file_path)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_checkpoint(model, file_path):\n",
    "    \"\"\"Load a checkpiont.\"\"\"\n",
    "    # I'm using the state dict to store performance of the model with\n",
    "    # that state, but PyTorch doesn't like unknown keys in its\n",
    "    # state dicts. Here's I'm just removing the that key and then\n",
    "    # loading the model's state.\n",
    "    state_dict = torch.load(file_path)\n",
    "    state_dict.pop('checkpoint_metric')\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def compute_loss_and_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Helper function to predict loss and calculate accuracy.\n",
    "\n",
    "    Args:\n",
    "        y_pred: A PyTorch tensor with shape `(batch_size, seq_length,\n",
    "            vocab_size)`.\n",
    "        y_true: A PyTorch tensor with shape `(batch_size, seq_length)`.\n",
    "\n",
    "    Returns:\n",
    "        loss: A tensor containing the cross entropy loss.\n",
    "        accuracy: A float containing the accuracy of the predictions,\n",
    "            assuming we predict the most likely word.\n",
    "\n",
    "    \"\"\"\n",
    "    # Flatten `y_pred` from `(batch_size, seq_length, vocab_size)` to\n",
    "    # `(batch_size * seq_length, vocab_size)`, a shape compatible with\n",
    "    # the loss function.\n",
    "    y_pred_flat = y_pred.view(-1, y_pred.size(2))\n",
    "    # Similarly, reshape y_true from `(batch_size, seq_length)` to\n",
    "    # `(batch_size * seq_length)`.\n",
    "    y_true_flat = y_true.view(-1)\n",
    "    loss = criterion(y_pred_flat, y_true_flat)\n",
    "    accuracy = accuracy_score(y_true_flat, y_pred_flat.argmax(dim=-1))\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "class BooksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, books_path, embeddings_path, sequence_length=50, val_ratio=0, test_ratio=0):\n",
    "        \"\"\"\n",
    "        A `Dataset` subclass that loads and prepares a dataset from \n",
    "        all the text files in a directory.\n",
    "        \n",
    "        Because this class implements `__len__` and `__getitem__`, it's\n",
    "        instances are sequences which `DataLoader` can index into to \n",
    "        generate mini-batches.\n",
    "        \n",
    "        Args:\n",
    "            books_path: The path to directory containing the text files. \n",
    "            embeddings_path: The path to pretrained embeddings file.\n",
    "            sequence_length: The length of the sequences to train on.\n",
    "            val_ratio: The proportion of the data to set a side for \n",
    "                validation. \n",
    "            test_ratio: The proportion of the data to set a side for \n",
    "                testing.\n",
    "                \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Read in all text documents in the folder at `books_path` and\n",
    "        # and join them into a single string.\n",
    "        glob_path = os.path.join(books_path, '*.txt')\n",
    "        texts = []\n",
    "        for file_path in glob(glob_path):\n",
    "            with open(file_path) as f:\n",
    "                text = f.read()\n",
    "                text = text.lower()\n",
    "                # Select lines that are longer than 50 chars or that\n",
    "                # start with quotation marks. Other lines are likely\n",
    "                # things like chapter titles, et cetera.\n",
    "                lines = re.findall(r'^(?:.{50}.*$)|(?:[\"“”].*$)',\n",
    "                                   text, flags=re.MULTILINE)\n",
    "                lines = [line.strip() for line in lines]\n",
    "                text = '\\n'.join(lines)\n",
    "                texts.append(text)\n",
    "        text = '\\n'.join(texts)\n",
    "\n",
    "        # Cut the text into `sequence_length` sized chunks.\n",
    "        X_tokens, y_tokens = text_to_tokens(text, sequence_length)\n",
    "\n",
    "        # Generate the embedding matrix and associated maps.\n",
    "        _, self.token_to_id, self.embedding_matrix = get_embeddings(\n",
    "            X_tokens, embeddings_path)\n",
    "        self.id_to_token = {token_id: token for token, token_id in self.token_to_id.items()}\n",
    "\n",
    "        # Pytorch wants these as LongTensors.\n",
    "        X_ids = tokens_to_ids(X_tokens, self.token_to_id)\n",
    "        X_ids = torch.LongTensor(X_ids)\n",
    "        y_ids = tokens_to_ids(y_tokens, self.token_to_id)\n",
    "        y_ids = torch.LongTensor(y_ids)\n",
    "\n",
    "        # Do a train/val/test split.\n",
    "        train_indicies, val_indicies, test_indicies = get_train_val_test_indices(\n",
    "            len(X_tokens), val_ratio, test_ratio)\n",
    "        self.X_train_ids = X_ids[train_indicies]\n",
    "        self.y_train_ids = y_ids[train_indicies]\n",
    "        self.X_val_ids = X_ids[val_indicies]\n",
    "        self.y_val_ids = y_ids[val_indicies]\n",
    "        self.X_test_ids = X_ids[test_indicies]\n",
    "        self.y_test_ids = y_ids[test_indicies]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_train_ids[index], self.y_train_ids[index]\n",
    "\n",
    "\n",
    "class NextWordPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, recurrent_layers=1,\n",
    "                 train_embeddings=False, recur_size=256):\n",
    "        \"\"\"\n",
    "        A neural net that predicts the next word at each time step.\n",
    "\n",
    "        Args:\n",
    "            embedding_matrix: A torch tensor or numpy array where each\n",
    "                row is the embedding vector of a word.\n",
    "            recurrent_layers: The number of LSTM layers to use. Defaults\n",
    "                to `1`.\n",
    "            train_embeddings: Whether or not to further train the\n",
    "                embedding weights or fix them in place. Defaults to\n",
    "                `False`.\n",
    "            recur_size: The number of dimensions in recurrent state.\n",
    "                Defaults to `256`.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create embedding layer and initialize its weights to passed\n",
    "        # in embedding_matrix.\n",
    "        if not isinstance(embedding_matrix, torch.Tensor):\n",
    "            embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "        vocab_size = embedding_matrix.size(0)\n",
    "        embedding_size = embedding_matrix.size(1)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embeddings.requires_grad = train_embeddings\n",
    "\n",
    "        # Setup LSTM layer.\n",
    "        self.recur_state = None\n",
    "        self.recur_size = recur_size\n",
    "        self.recurrent_layers = recurrent_layers\n",
    "        self.lstm = nn.LSTM(embedding_size, recur_size,\n",
    "                            num_layers=recurrent_layers, batch_first=True)\n",
    "\n",
    "        # Setup fully connected word predictor.\n",
    "        self.word_predictor = nn.Linear(recur_size, vocab_size)\n",
    "\n",
    "    def init_recur_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Return an empty hidden state for the recurrent layer.\n",
    "\n",
    "        Args:\n",
    "             batch_size (int): The number of training examples in\n",
    "                each mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            (tuple): A tuple of torch tensors each with the shape\n",
    "                `(num_recur_layers, batch_size, recur_size)`.\n",
    "\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.recurrent_layers, batch_size, self.recur_size),\n",
    "                torch.zeros(self.recurrent_layers, batch_size, self.recur_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For each word, predict the probabilities for the next word.\n",
    "\n",
    "        Args:\n",
    "            x: A torch tensor with shape `(batch_size, seq_length)`.\n",
    "                Its elements are word indices that map to word vectors\n",
    "                in the `embedding_matrix`.\n",
    "\n",
    "        Returns:\n",
    "            A torch tensor with shape `(batch_size, seq_length, vocab_size)`.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        # Passing `x` through the embedding layer looks up the word\n",
    "        # indices and replaces them their corresponding embedding\n",
    "        # vectors. This changes its shape from `(batch_length, seq_length)`\n",
    "        # to `(batch_length, seq_length, embedding_size)`.\n",
    "        x = self.embeddings(x)\n",
    "        # Passing `x` through the LSTM layer changes its shape from\n",
    "        # `(batch_length, seq_length, embedding_size)` to\n",
    "        # `(batch_length, seq_length, recur_size)`\n",
    "        x, self.recur_state = self.lstm(x, self.recur_state)\n",
    "        # I'm calling `x.contiguous()` because otherwise the `x.view`\n",
    "        # call below fails and prints an error message telling me\n",
    "        # to call it.\n",
    "        x = x.contiguous()\n",
    "        # Flatten out the result so it has shape\n",
    "        # `(batch_length * seq_length, recur_size)`.\n",
    "        x = x.view(-1, self.recur_size)\n",
    "        # Apply a fully connected layer to each element of the flattened\n",
    "        # `x`, which are `recur_size` vectors representing words. This\n",
    "        # Produces a tensor of shape `(batch_length * seq_length,\n",
    "        # vocab_size)`.\n",
    "        x = self.word_predictor(x)\n",
    "        # Apply softmax activation over the last dimension, turning them\n",
    "        # into probability distributions over words.\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # Before returning, reshape to `(batch_size, seq_length,\n",
    "        # vocab_size)`.\n",
    "        return x.view(batch_size, seq_length, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'https://www.dropbox.com/s/5xeq6ijqfw9xlh9/data.zip?dl=1'\n",
    "SAVE_PATH = 'data.zip'\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    # Download the data zip file.\n",
    "    response = requests.get(DATA_URL, stream=True)\n",
    "    with open(SAVE_PATH, 'wb') as f:\n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "    # Unzip the file.\n",
    "    with zipfile.ZipFile(SAVE_PATH, 'r') as z:\n",
    "        z.extractall('')\n",
    "    # Delete the zip file.\n",
    "    shutil.rmtree('__MACOSX')\n",
    "    os.remove(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOOKS_PATH = 'data/texts'\n",
    "EMEDDINGS_PATH = 'data/glove.6B.200d.txt'\n",
    "CHECKPOINT_PATH = 'data/checkpoint.pickle'\n",
    "DATASET_SAVE_PATH = 'data/dataset.pickle'\n",
    "MODEL_SAVE_PATH = 'data/model.pickle'\n",
    "\n",
    "HIDDEN_LAYERS = 1\n",
    "TRAIN_EMBEDDINGS = True\n",
    "USE_CHECKPOINTING = False\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LOG_FREQ = 100  # In mini-batches.\n",
    "LOAD_CACHED_DATASET = True  # Turn off if you use your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and datset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if LOAD_CACHED_DATASET and os.path.exists(DATASET_SAVE_PATH):\n",
    "    books_dataset = joblib.load(DATASET_SAVE_PATH)\n",
    "else:\n",
    "    books_dataset = BooksDataset(BOOKS_PATH, EMEDDINGS_PATH, val_ratio=0, test_ratio=0)\n",
    "    joblib.dump(books_dataset, DATASET_SAVE_PATH)\n",
    "dataloader = DataLoader(books_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NextWordPredictor(books_dataset.embedding_matrix, \n",
    "                          recurrent_layers=HIDDEN_LAYERS, \n",
    "                          train_embeddings=TRAIN_EMBEDDINGS)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0.\n",
      "Mini-batch: 100/409 Loss: 6.17821 Accuracy: 0.14781\n",
      "Mini-batch: 200/409 Loss: 5.26532 Accuracy: 0.19967\n",
      "Mini-batch: 300/409 Loss: 5.08472 Accuracy: 0.21158\n",
      "Mini-batch: 400/409 Loss: 4.98249 Accuracy: 0.21980\n",
      "Epoch 0 completed in 4 minutes 16 seconds.\n",
      "\n",
      "Starting epoch 1.\n",
      "Mini-batch: 100/409 Loss: 4.57262 Accuracy: 0.23451\n",
      "Mini-batch: 200/409 Loss: 4.54959 Accuracy: 0.23888\n",
      "Mini-batch: 300/409 Loss: 4.55398 Accuracy: 0.23779\n",
      "Mini-batch: 400/409 Loss: 4.54612 Accuracy: 0.24056\n",
      "Epoch 1 completed in 3 minutes 57 seconds.\n",
      "\n",
      "Starting epoch 2.\n",
      "Mini-batch: 100/409 Loss: 4.07988 Accuracy: 0.26506\n",
      "Mini-batch: 200/409 Loss: 4.15897 Accuracy: 0.25848\n",
      "Mini-batch: 300/409 Loss: 4.17456 Accuracy: 0.26027\n",
      "Mini-batch: 400/409 Loss: 4.21322 Accuracy: 0.25613\n",
      "Epoch 2 completed in 4 minutes 0 seconds.\n",
      "\n",
      "Starting epoch 3.\n",
      "Mini-batch: 100/409 Loss: 3.73409 Accuracy: 0.29273\n",
      "Mini-batch: 200/409 Loss: 3.82359 Accuracy: 0.28371\n",
      "Mini-batch: 300/409 Loss: 3.89030 Accuracy: 0.27799\n",
      "Mini-batch: 400/409 Loss: 3.93445 Accuracy: 0.27597\n",
      "Epoch 3 completed in 4 minutes 6 seconds.\n",
      "\n",
      "Starting epoch 4.\n",
      "Mini-batch: 100/409 Loss: 3.47444 Accuracy: 0.31946\n",
      "Mini-batch: 200/409 Loss: 3.57887 Accuracy: 0.30778\n",
      "Mini-batch: 300/409 Loss: 3.64759 Accuracy: 0.30030\n",
      "Mini-batch: 400/409 Loss: 3.71942 Accuracy: 0.29291\n",
      "Epoch 4 completed in 4 minutes 13 seconds.\n",
      "\n",
      "Starting epoch 5.\n",
      "Mini-batch: 100/409 Loss: 3.28156 Accuracy: 0.34249\n",
      "Mini-batch: 200/409 Loss: 3.39430 Accuracy: 0.32788\n",
      "Mini-batch: 300/409 Loss: 3.46641 Accuracy: 0.32013\n",
      "Mini-batch: 400/409 Loss: 3.52657 Accuracy: 0.31464\n",
      "Epoch 5 completed in 4 minutes 1 seconds.\n",
      "\n",
      "Starting epoch 6.\n",
      "Mini-batch: 100/409 Loss: 3.11712 Accuracy: 0.36340\n",
      "Mini-batch: 200/409 Loss: 3.23527 Accuracy: 0.34830\n",
      "Mini-batch: 300/409 Loss: 3.32997 Accuracy: 0.33707\n",
      "Mini-batch: 400/409 Loss: 3.40431 Accuracy: 0.32652\n",
      "Epoch 6 completed in 4 minutes 5 seconds.\n",
      "\n",
      "Starting epoch 7.\n",
      "Mini-batch: 100/409 Loss: 2.99597 Accuracy: 0.38126\n",
      "Mini-batch: 200/409 Loss: 3.12588 Accuracy: 0.36224\n",
      "Mini-batch: 300/409 Loss: 3.21626 Accuracy: 0.35048\n",
      "Mini-batch: 400/409 Loss: 3.28167 Accuracy: 0.34152\n",
      "Epoch 7 completed in 4 minutes 17 seconds.\n",
      "\n",
      "Starting epoch 8.\n",
      "Mini-batch: 100/409 Loss: 2.89099 Accuracy: 0.39682\n",
      "Mini-batch: 200/409 Loss: 3.02400 Accuracy: 0.37718\n",
      "Mini-batch: 300/409 Loss: 3.12306 Accuracy: 0.36124\n",
      "Mini-batch: 400/409 Loss: 3.20184 Accuracy: 0.35190\n",
      "Epoch 8 completed in 4 minutes 17 seconds.\n",
      "\n",
      "Starting epoch 9.\n",
      "Mini-batch: 100/409 Loss: 2.82622 Accuracy: 0.40568\n",
      "Mini-batch: 200/409 Loss: 2.94669 Accuracy: 0.38782\n",
      "Mini-batch: 300/409 Loss: 3.04375 Accuracy: 0.37366\n",
      "Mini-batch: 400/409 Loss: 3.11772 Accuracy: 0.36284\n",
      "Epoch 9 completed in 4 minutes 18 seconds.\n",
      "\n",
      "Starting epoch 10.\n",
      "Mini-batch: 100/409 Loss: 2.76428 Accuracy: 0.41570\n",
      "Mini-batch: 200/409 Loss: 2.88966 Accuracy: 0.39533\n",
      "Mini-batch: 300/409 Loss: 2.97220 Accuracy: 0.38092\n",
      "Mini-batch: 400/409 Loss: 3.06167 Accuracy: 0.36999\n",
      "Epoch 10 completed in 4 minutes 13 seconds.\n",
      "\n",
      "Starting epoch 11.\n",
      "Mini-batch: 100/409 Loss: 2.71789 Accuracy: 0.42142\n",
      "Mini-batch: 200/409 Loss: 2.82729 Accuracy: 0.40366\n",
      "Mini-batch: 300/409 Loss: 2.91889 Accuracy: 0.38897\n",
      "Mini-batch: 400/409 Loss: 3.01240 Accuracy: 0.37641\n",
      "Epoch 11 completed in 4 minutes 17 seconds.\n",
      "\n",
      "Starting epoch 12.\n",
      "Mini-batch: 100/409 Loss: 2.66795 Accuracy: 0.42930\n",
      "Mini-batch: 200/409 Loss: 2.79293 Accuracy: 0.40836\n",
      "Mini-batch: 300/409 Loss: 2.88051 Accuracy: 0.39440\n",
      "Mini-batch: 400/409 Loss: 2.95707 Accuracy: 0.38268\n",
      "Epoch 12 completed in 4 minutes 21 seconds.\n",
      "\n",
      "Starting epoch 13.\n",
      "Mini-batch: 100/409 Loss: 2.62468 Accuracy: 0.43561\n",
      "Mini-batch: 200/409 Loss: 2.75437 Accuracy: 0.41399\n",
      "Mini-batch: 300/409 Loss: 2.83939 Accuracy: 0.39987\n",
      "Mini-batch: 400/409 Loss: 2.93802 Accuracy: 0.38592\n",
      "Epoch 13 completed in 4 minutes 45 seconds.\n",
      "\n",
      "Starting epoch 14.\n",
      "Mini-batch: 100/409 Loss: 2.60260 Accuracy: 0.43929\n",
      "Mini-batch: 200/409 Loss: 2.71552 Accuracy: 0.41871\n",
      "Mini-batch: 300/409 Loss: 2.81031 Accuracy: 0.40379\n",
      "Mini-batch: 400/409 Loss: 2.90348 Accuracy: 0.38990\n",
      "Epoch 14 completed in 4 minutes 29 seconds.\n",
      "\n",
      "Starting epoch 15.\n",
      "Mini-batch: 100/409 Loss: 2.56899 Accuracy: 0.44329\n",
      "Mini-batch: 200/409 Loss: 2.69327 Accuracy: 0.42357\n",
      "Mini-batch: 300/409 Loss: 2.79229 Accuracy: 0.40583\n",
      "Mini-batch: 400/409 Loss: 2.86993 Accuracy: 0.39419\n",
      "Epoch 15 completed in 4 minutes 34 seconds.\n",
      "\n",
      "Starting epoch 16.\n",
      "Mini-batch: 100/409 Loss: 2.56246 Accuracy: 0.44343\n",
      "Mini-batch: 200/409 Loss: 2.66653 Accuracy: 0.42696\n",
      "Mini-batch: 300/409 Loss: 2.75825 Accuracy: 0.41073\n",
      "Mini-batch: 400/409 Loss: 2.84832 Accuracy: 0.39809\n",
      "Epoch 16 completed in 4 minutes 29 seconds.\n",
      "\n",
      "Starting epoch 17.\n",
      "Mini-batch: 100/409 Loss: 2.54148 Accuracy: 0.44745\n",
      "Mini-batch: 200/409 Loss: 2.64161 Accuracy: 0.42988\n",
      "Mini-batch: 300/409 Loss: 2.74886 Accuracy: 0.41222\n",
      "Mini-batch: 400/409 Loss: 2.82821 Accuracy: 0.39932\n",
      "Epoch 17 completed in 4 minutes 52 seconds.\n",
      "\n",
      "Starting epoch 18.\n",
      "Mini-batch: 100/409 Loss: 2.51443 Accuracy: 0.45066\n",
      "Mini-batch: 200/409 Loss: 2.63146 Accuracy: 0.43167\n",
      "Mini-batch: 300/409 Loss: 2.73317 Accuracy: 0.41374\n",
      "Mini-batch: 400/409 Loss: 2.81603 Accuracy: 0.40248\n",
      "Epoch 18 completed in 4 minutes 46 seconds.\n",
      "\n",
      "Starting epoch 19.\n",
      "Mini-batch: 100/409 Loss: 2.50510 Accuracy: 0.45241\n",
      "Mini-batch: 200/409 Loss: 2.62498 Accuracy: 0.43122\n",
      "Mini-batch: 300/409 Loss: 2.71825 Accuracy: 0.41586\n",
      "Mini-batch: 400/409 Loss: 2.79424 Accuracy: 0.40354\n",
      "Epoch 19 completed in 5 minutes 14 seconds.\n",
      "\n",
      "Starting epoch 20.\n",
      "Mini-batch: 100/409 Loss: 2.49094 Accuracy: 0.45517\n",
      "Mini-batch: 200/409 Loss: 2.60761 Accuracy: 0.43393\n",
      "Mini-batch: 300/409 Loss: 2.71571 Accuracy: 0.41518\n",
      "Mini-batch: 400/409 Loss: 2.76682 Accuracy: 0.40768\n",
      "Epoch 20 completed in 5 minutes 19 seconds.\n",
      "\n",
      "Starting epoch 21.\n",
      "Mini-batch: 100/409 Loss: 2.48606 Accuracy: 0.45460\n",
      "Mini-batch: 200/409 Loss: 2.59801 Accuracy: 0.43519\n",
      "Mini-batch: 300/409 Loss: 2.69053 Accuracy: 0.41939\n",
      "Mini-batch: 400/409 Loss: 2.77675 Accuracy: 0.40617\n",
      "Epoch 21 completed in 5 minutes 25 seconds.\n",
      "\n",
      "Starting epoch 22.\n",
      "Mini-batch: 100/409 Loss: 2.47120 Accuracy: 0.45698\n",
      "Mini-batch: 200/409 Loss: 2.59039 Accuracy: 0.43609\n",
      "Mini-batch: 300/409 Loss: 2.68505 Accuracy: 0.41982\n",
      "Mini-batch: 400/409 Loss: 2.76446 Accuracy: 0.40690\n",
      "Epoch 22 completed in 5 minutes 22 seconds.\n",
      "\n",
      "Starting epoch 23.\n",
      "Mini-batch: 100/409 Loss: 2.47650 Accuracy: 0.45565\n",
      "Mini-batch: 200/409 Loss: 2.57821 Accuracy: 0.43763\n",
      "Mini-batch: 300/409 Loss: 2.67468 Accuracy: 0.42180\n",
      "Mini-batch: 400/409 Loss: 2.75032 Accuracy: 0.41018\n",
      "Epoch 23 completed in 5 minutes 25 seconds.\n",
      "\n",
      "Starting epoch 24.\n",
      "Mini-batch: 100/409 Loss: 2.45914 Accuracy: 0.45845\n",
      "Mini-batch: 200/409 Loss: 2.57799 Accuracy: 0.43796\n",
      "Mini-batch: 300/409 Loss: 2.66003 Accuracy: 0.42337\n",
      "Mini-batch: 400/409 Loss: 2.75055 Accuracy: 0.40800\n",
      "Epoch 24 completed in 5 minutes 44 seconds.\n",
      "\n",
      "Starting epoch 25.\n",
      "Mini-batch: 100/409 Loss: 2.45199 Accuracy: 0.45842\n",
      "Mini-batch: 200/409 Loss: 2.58003 Accuracy: 0.43619\n",
      "Mini-batch: 300/409 Loss: 2.66088 Accuracy: 0.42299\n",
      "Mini-batch: 400/409 Loss: 2.74683 Accuracy: 0.40972\n",
      "Epoch 25 completed in 5 minutes 32 seconds.\n",
      "\n",
      "Starting epoch 26.\n",
      "Mini-batch: 100/409 Loss: 2.45592 Accuracy: 0.45827\n",
      "Mini-batch: 200/409 Loss: 2.56217 Accuracy: 0.43956\n",
      "Mini-batch: 300/409 Loss: 2.65910 Accuracy: 0.42297\n",
      "Mini-batch: 400/409 Loss: 2.73360 Accuracy: 0.41217\n",
      "Epoch 26 completed in 5 minutes 40 seconds.\n",
      "\n",
      "Starting epoch 27.\n",
      "Mini-batch: 100/409 Loss: 2.45134 Accuracy: 0.45855\n",
      "Mini-batch: 200/409 Loss: 2.56285 Accuracy: 0.44012\n",
      "Mini-batch: 300/409 Loss: 2.65466 Accuracy: 0.42332\n",
      "Mini-batch: 400/409 Loss: 2.73700 Accuracy: 0.41069\n",
      "Epoch 27 completed in 5 minutes 53 seconds.\n",
      "\n",
      "Starting epoch 28.\n",
      "Mini-batch: 100/409 Loss: 2.44003 Accuracy: 0.46054\n",
      "Mini-batch: 200/409 Loss: 2.56528 Accuracy: 0.43849\n",
      "Mini-batch: 300/409 Loss: 2.65670 Accuracy: 0.42302\n",
      "Mini-batch: 400/409 Loss: 2.73779 Accuracy: 0.41079\n",
      "Epoch 28 completed in 5 minutes 59 seconds.\n",
      "\n",
      "Starting epoch 29.\n",
      "Mini-batch: 100/409 Loss: 2.45086 Accuracy: 0.45713\n",
      "Mini-batch: 200/409 Loss: 2.55545 Accuracy: 0.43958\n",
      "Mini-batch: 300/409 Loss: 2.65632 Accuracy: 0.42460\n",
      "Mini-batch: 400/409 Loss: 2.72527 Accuracy: 0.41250\n",
      "Epoch 29 completed in 6 minutes 12 seconds.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Starting epoch {epoch}.')\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    # Iterating over a DataLoader instance iterates over the whole\n",
    "    # dataset in `batch_size` sized chunks.\n",
    "    for i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "        # Switch to training mode (shouldn't matter here, but good\n",
    "        # practice as some layers like dropout behave differently in\n",
    "        # training and evaluation modes).\n",
    "        model.train()\n",
    "        # Clear the gradients from previous mini-batch.\n",
    "        optimizer.zero_grad()\n",
    "        # Reset the recurrent state. We don't want to retain this\n",
    "        # between batches as the text segments aren't going to be\n",
    "        # contiguous.\n",
    "        model.recur_state = model.init_recur_state(batch_X.size(0))\n",
    "        # Predict the next word at each time step. `y_pred` has shape\n",
    "        # `(batch_size, seq_length, vocab_size)`.\n",
    "        y_pred = model(batch_X)\n",
    "        loss, accuracy = compute_loss_and_accuracy(y_pred, batch_y)\n",
    "        # Calculate gradients.\n",
    "        loss.backward()\n",
    "        # Update weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Periodically print the loss and prediction accuracy. Usually\n",
    "        # with a language model you'd also show perplexity, but as it's\n",
    "        # a function of our cross entropy loss and not that intuitive,\n",
    "        # I've elected not to.\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += accuracy\n",
    "        if i % LOG_FREQ == LOG_FREQ - 1:\n",
    "            average_loss = running_loss / LOG_FREQ\n",
    "            average_accuracy = running_accuracy / LOG_FREQ\n",
    "            print(f'Mini-batch: {i + 1}/{len(dataloader)} '\n",
    "                  f'Loss: {average_loss:.5f} Accuracy: {average_accuracy:.5f}')\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "    # Log elapsed_time for the epoch.\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    print(f'Epoch {epoch} completed in {elapsed_time // 60:.0f} minutes '\n",
    "          f'{elapsed_time % 60:.0f} seconds.\\n')\n",
    "\n",
    "    # If checkpointing, check to see if loss on the validation set is\n",
    "    # lower than at the previous checkpoint. If so, save the state of\n",
    "    # the model. This is less useful on this task as we also want the\n",
    "    # model to memorize the dataset here, but on other tasks it's\n",
    "    # to save a state of the model before it starts to over fit.\n",
    "    if USE_CHECKPOINTING:\n",
    "        if not books_dataset.X_val_ids.size(0):\n",
    "            raise Exception(\"Can't checkpoint without validation data!\")\n",
    "        with torch.no_grad:\n",
    "            model.recur_state = model.init_recur_state(\n",
    "                books_dataset.X_val_ids.size(0))\n",
    "            y_pred = model(books_dataset.X_val_ids)\n",
    "            val_loss, val_accuracy = compute_loss_and_accuracy(\n",
    "                y_pred, books_dataset.y_val_ids)\n",
    "            print(f'Val Loss: {val_loss:.5f} Val Accuracy: {val_accuracy:.5f}')\n",
    "            if save_checkpoint(model, val_loss, CHECKPOINT_PATH, higher_is_better=False):\n",
    "                print('Validation set performance improved, saving checkpoint.\\n')\n",
    "            else:\n",
    "                print('Validation set performance did NOT improve.\\n')\n",
    "\n",
    "# If checkpointing, load the model state with the lowest validation loss.\n",
    "if USE_CHECKPOINTING:\n",
    "    load_checkpoint(model, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional save/restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Save the complete model object for easy reloading.\n",
    "joblib.dump(model, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the model and the dataset.\n",
    "model = joblib.load(MODEL_SAVE_PATH)\n",
    "books_dataset = joblib.load(DATASET_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry stepped onto y.t . 's plank . she is not too scared of the people who feel anymore .\n",
      "\n",
      "“ okay , sasha . you ’ re infected with your lawyer , we ’ re all dead rumor ? ”\n",
      "\n",
      "“ i ’ m not asking you to do , ” randy says . “ i ’ m not sure if you like. ”\n",
      "\n",
      "“ if you ’ re going to sponsor a stable currency , ” goto dengo says . “ we are going to be a major drum heap drum into the seafloor , which is now calls to the states .\n",
      "\n",
      "“ what is the site ? ”\n",
      "\n",
      "“ yes , sir. ”\n",
      "\n",
      "“ i ’ m not allowed to do this. ” the lieutenant ’ s smile turns around , and opens up his briefcase . “ i ’ m not going to be back. ”\n",
      "\n",
      "“ i ’ m not asking you to do , ” avi says . “ i ’ m going to be troublesome . i ’ m going to get the fuck out of jail. ”\n",
      "\n",
      "“ sir ! yes , sir ! ”\n",
      "\n",
      "“ i ’ m not asking you to justify the , ” says the skipper . “ i ’ m not sure if you ’ re not limited to congratulate your passport control over the convoy , i ’ ll be wanting to make a better sense , ” he says .\n",
      "\n",
      "“ i ’ ll be fucked ! ” nina says . “ i ’ m not gon na interview over to the level of the country that you have a bad idea of that , but i do not understand. ”\n",
      "\n",
      "“ i ’ m not the only one who ’ ll be putting up with a level of thoughts of other people , ” shaftoe says . “ i ’ m not gon na a merchantman . a very bad question , i do not understand. ”\n",
      "\n",
      "“ it is a problem with me that i was going to be invaded now , ” he says . “ i ’ m not sure if you ’ re not smuggling medicine . the germans have fewer than a mere twenty - year - old american boy has a gun in a pathetic ball . but if you ’ re going\n"
     ]
    }
   ],
   "source": [
    "SEED = \"Harry stepped onto Y.T.'s plank.\"\n",
    "NUM_TO_GENERATE = 400\n",
    "\n",
    "seed_tokens = [tuple(TextBlob(SEED.lower()).tokens)]\n",
    "seed_ids = tokens_to_ids(seed_tokens, books_dataset.token_to_id)\n",
    "seed_ids = torch.LongTensor(seed_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # Clear the hidden state of the model.\n",
    "    model.recur_state = model.init_recur_state(1)\n",
    "    # Read in seed.\n",
    "    y_pred = model(seed_ids)\n",
    "    # Slice of the predicted probs for the last element of the output\n",
    "    # (which wasn't in the seed) and predict the token with highest\n",
    "    # probability. The result is tensor with shape `(1, 1)`.\n",
    "    last_token_pred = y_pred[:, -1:, :].argmax(-1)\n",
    "\n",
    "    # Generate more text by feeding in the previously predicted word\n",
    "    # and then predicting the next.\n",
    "    generated = [last_token_pred]\n",
    "    for _ in range(NUM_TO_GENERATE - 1):\n",
    "        next_word_pred = model(generated[-1])\n",
    "        generated.append(next_word_pred.argmax(-1))\n",
    "\n",
    "# Combine the ids from the seed and the generated text.\n",
    "combined_ids = list(tensor.item() for tensor in seed_ids[0])\n",
    "combined_ids.extend(tensor.item() for tensor in generated)\n",
    "\n",
    "# Map the token ids back to tokens.\n",
    "combined_tokens = ids_to_tokens(combined_ids, books_dataset.id_to_token)\n",
    "\n",
    "# Turn it back into a string.\n",
    "generated_text = ' '.join(combined_tokens)\n",
    "generated_text = generated_text.replace(' NEWLINE ', '\\n\\n')\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
